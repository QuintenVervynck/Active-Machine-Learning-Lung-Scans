{"cells":[{"cell_type":"markdown","metadata":{"id":"RSPdntIhzCWH"},"source":["# Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCAHMAZwzCWL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.utils.data as data\n","import torchvision\n","from torchvision import transforms\n","\n","from keras.layers import Input, Conv2D, MaxPooling2D\n","from keras.layers import Dense, Flatten\n","from keras.models import Model\n","import matplotlib.pyplot as plt\n","import pickle"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODP1c0CUzrIS","executionInfo":{"status":"ok","timestamp":1667400752123,"user_tz":-60,"elapsed":3815,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}},"outputId":"5aab7149-8ba5-412c-c493-80ed2db43739"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"qKL07NKBzCWO","outputId":"1664f9f6-ce8b-446c-b7cc-00163205f150","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667400852446,"user_tz":-60,"elapsed":1938,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["batch size 64\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["BATCH_SIZE = 64 \n","print(\"batch size\", BATCH_SIZE)\n","LEARNING_RATE = 0.003\n","TRAIN_DATA_PATH = \"/content/drive/MyDrive/ML/ActiveLearning_ImageClassification/categories_fruits_big/train\"\n","TEST_DATA_PATH = \"/content/drive/MyDrive/ML/ActiveLearning_ImageClassification/categories_fruits_big/test\"\n","# TRANSFORM_IMG = transforms.Compose([\n","#     transforms.Resize(256),\n","#     transforms.CenterCrop(256),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","#                          std=[0.229, 0.224, 0.225] )\n","#     ])\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# transform_test = transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","# ])\n","\n","\n","transform_test = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","\n","train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_train)\n","train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n","test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_test)\n","test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n","\n","trainset = train_data\n","trainloader = train_data_loader\n","testset = test_data\n","testloader = test_data_loader"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"SvWSek0yzCWP","outputId":"2a77b28b-7b07-4d0f-9ba8-d56224905dc4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667400867491,"user_tz":-60,"elapsed":237,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of train samples:  75\n","Number of test samples:  60\n","Detected Classes are:  {'apple': 0, 'banana': 1, 'onion': 2, 'pear': 3, 'pepper': 4, 'pineapple': 5, 'plum': 6, 'potato': 7, 'strawberry': 8, 'tomato': 9}\n"]}],"source":["print(\"Number of train samples: \", len(train_data))\n","print(\"Number of test samples: \", len(test_data))\n","print(\"Detected Classes are: \", train_data.class_to_idx) # classes are detected by folder structure\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"A9i9m6gSzCWP","outputId":"483bfff0-d1a2-41b2-eb2e-f052ce28272e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667400877075,"user_tz":-60,"elapsed":363,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["num classes =  10\n"]}],"source":["print(\"num classes = \", len(train_data.class_to_idx))"]},{"cell_type":"markdown","metadata":{"id":"JSkiYxiXzCWQ"},"source":["# Create Model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"l5GkNrDMzCWR","executionInfo":{"status":"ok","timestamp":1667400880326,"user_tz":-60,"elapsed":243,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[],"source":["class Net3(nn.Module):\n","    def __init__(self):\n","        super(Net3, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 28, 3,padding=1)\n","        self.bn1 = nn.BatchNorm2d(28)\n","        self.conv2 = nn.Conv2d(28, 35, 3,padding=1)\n","        self.bn2 = nn.BatchNorm2d(35)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.drop1 = nn.Dropout(p=0.2)\n","        self.conv3 = nn.Conv2d(35, 16, 3,padding=1)\n","        self.bn3 = nn.BatchNorm2d(16)\n","        \n","        self.fc1 = nn.Linear(16 * 8 * 8, 100)\n","        self.drop2 = nn.Dropout(p=0.3)\n","        self.fc2 = nn.Linear(100, 30)\n","        self.bn4 = nn.BatchNorm1d(30)\n","        self.fc3 = nn.Linear(30, 10)\n","\n","    def forward(self, x):\n","        x = self.bn1(F.relu(self.conv1(x)))\n","        x = self.drop1(x)\n","        x = self.bn2(F.relu(self.conv2(x)))\n","        x = self.pool(x)\n","        x = self.bn3(F.relu(self.conv3(x)))\n","        x = self.pool(x)\n","        x = x.view(-1, 16 * 8 * 8)\n","        x = self.drop1(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.bn4(F.relu(self.fc2(x)))\n","        x = self.fc3(x)\n","    \n","        return F.log_softmax(x, dim=0)\n"]},{"cell_type":"markdown","metadata":{"id":"xkjySLC1zCWS"},"source":["# Simulating Active Learning"]},{"cell_type":"markdown","metadata":{"id":"wYucG2ruzCWS"},"source":["# Active Model: Minimum Largest Difference"]},{"cell_type":"markdown","metadata":{"id":"SvnHrK2fzCWT"},"source":["# Plot with Test Error"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Xm_MbcZszCWT","executionInfo":{"status":"ok","timestamp":1667400887648,"user_tz":-60,"elapsed":312,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[],"source":["def compute_test_acc(model, testloader):\n","    # Test the model\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        count = 0\n","        for images, labels in testloader:\n","            count += 1\n","            images = Variable(images)\n","            labels = Variable(labels)\n","\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return (correct / total)\n","\n","def compute_train_acc(model, trainloader):\n","    # Test the model\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        count = 0\n","        for images, labels in trainloader:\n","            count += 1\n","            images = Variable(images)\n","            labels = Variable(labels)\n","\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return (correct / total)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Ps89boc4zCWU","executionInfo":{"status":"ok","timestamp":1667400891886,"user_tz":-60,"elapsed":334,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}}},"outputs":[],"source":["# Find optimal_batch\n","def uncertainty_metric_max_min_difference_selection(model, trainloader):\n","    batch_evaluation = []\n","    for i, (images, labels) in enumerate(trainloader):\n","        outputs = model(images)\n","        batch_differences = []\n","        for j in range(len(outputs)):\n","            min_prob = min(outputs[j])\n","            max_prob = max(outputs[j])\n","            diff = float(max_prob - min_prob)\n","            batch_differences.append(diff)\n","        batch_mean = np.mean(batch_differences)\n","        batch_evaluation.append(batch_mean)\n","    batch_evaluation = np.array(batch_evaluation)\n","    k = 20\n","#     largest = list((-batch_evaluation).argsort()[:k])\n","    smallest = list(np.argsort(batch_evaluation)[:k])\n","    return smallest"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"2Gr3pYc6zCWU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667402339900,"user_tz":-60,"elapsed":1443282,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}},"outputId":"2d5449ab-68b5-47f4-8da2-80c2d4878755"},"outputs":[{"output_type":"stream","name":"stdout","text":["120753\n","2\n","epoch:  0\n","epoch:  1\n","epoch:  2\n","epoch:  3\n","epoch:  4\n","epoch:  5\n","epoch:  6\n","epoch:  7\n","epoch:  8\n","epoch:  9\n","epoch:  10\n","epoch:  11\n","epoch:  12\n","epoch:  13\n","epoch:  14\n","epoch:  15\n","epoch:  16\n","epoch:  17\n","epoch:  18\n","epoch:  19\n","epoch:  20\n","epoch:  21\n","epoch:  22\n","epoch:  23\n","epoch:  24\n","epoch:  25\n","epoch:  26\n","epoch:  27\n","epoch:  28\n","epoch:  29\n","epoch:  30\n","epoch:  31\n","epoch:  32\n","epoch:  33\n","epoch:  34\n","epoch:  35\n","epoch:  36\n","epoch:  37\n","epoch:  38\n","epoch:  39\n","epoch:  40\n","epoch:  41\n","epoch:  42\n","epoch:  43\n","epoch:  44\n","epoch:  45\n","epoch:  46\n","epoch:  47\n","epoch:  48\n","epoch:  49\n","epoch:  50\n","epoch:  51\n","epoch:  52\n","epoch:  53\n","epoch:  54\n","epoch:  55\n","epoch:  56\n","epoch:  57\n","epoch:  58\n","epoch:  59\n","epoch:  60\n","epoch:  61\n","epoch:  62\n","epoch:  63\n","epoch:  64\n","epoch:  65\n","epoch:  66\n","epoch:  67\n","epoch:  68\n","epoch:  69\n","epoch:  70\n","epoch:  71\n","epoch:  72\n","epoch:  73\n","epoch:  74\n","epoch:  75\n","epoch:  76\n","epoch:  77\n","epoch:  78\n","epoch:  79\n","epoch:  80\n","epoch:  81\n","epoch:  82\n","epoch:  83\n","epoch:  84\n","epoch:  85\n","epoch:  86\n","epoch:  87\n","epoch:  88\n","epoch:  89\n","epoch:  90\n","epoch:  91\n","epoch:  92\n","epoch:  93\n","epoch:  94\n","epoch:  95\n","epoch:  96\n","epoch:  97\n","epoch:  98\n","epoch:  99\n","epoch:  100\n","epoch:  101\n","epoch:  102\n","epoch:  103\n","epoch:  104\n","epoch:  105\n","epoch:  106\n","epoch:  107\n","epoch:  108\n","epoch:  109\n","epoch:  110\n","epoch:  111\n","epoch:  112\n","epoch:  113\n","epoch:  114\n","epoch:  115\n","epoch:  116\n","epoch:  117\n","epoch:  118\n","epoch:  119\n","epoch:  120\n","epoch:  121\n","epoch:  122\n","epoch:  123\n","epoch:  124\n","epoch:  125\n","epoch:  126\n","epoch:  127\n","epoch:  128\n","epoch:  129\n","epoch:  130\n","epoch:  131\n","epoch:  132\n","epoch:  133\n","epoch:  134\n","epoch:  135\n","epoch:  136\n","epoch:  137\n","epoch:  138\n","epoch:  139\n","epoch:  140\n","epoch:  141\n","epoch:  142\n","epoch:  143\n","epoch:  144\n","epoch:  145\n","epoch:  146\n","epoch:  147\n","epoch:  148\n","epoch:  149\n","epoch:  150\n","epoch:  151\n","epoch:  152\n","epoch:  153\n","epoch:  154\n","epoch:  155\n","epoch:  156\n","epoch:  157\n","epoch:  158\n","epoch:  159\n","epoch:  160\n","epoch:  161\n","epoch:  162\n","epoch:  163\n","epoch:  164\n","epoch:  165\n","epoch:  166\n","epoch:  167\n","epoch:  168\n","epoch:  169\n","epoch:  170\n","epoch:  171\n","epoch:  172\n","epoch:  173\n","epoch:  174\n","epoch:  175\n","epoch:  176\n","epoch:  177\n","epoch:  178\n","epoch:  179\n","epoch:  180\n","epoch:  181\n","epoch:  182\n","epoch:  183\n","epoch:  184\n","epoch:  185\n","epoch:  186\n","epoch:  187\n","epoch:  188\n","epoch:  189\n","epoch:  190\n","epoch:  191\n","epoch:  192\n","epoch:  193\n","epoch:  194\n","epoch:  195\n","epoch:  196\n","epoch:  197\n","epoch:  198\n","epoch:  199\n","120753\n","2\n","epoch:  0\n","epoch:  1\n","epoch:  2\n","epoch:  3\n","epoch:  4\n","epoch:  5\n","epoch:  6\n","epoch:  7\n","epoch:  8\n","epoch:  9\n","epoch:  10\n","epoch:  11\n","epoch:  12\n","epoch:  13\n","epoch:  14\n","epoch:  15\n","epoch:  16\n","epoch:  17\n","epoch:  18\n","epoch:  19\n","epoch:  20\n","epoch:  21\n","epoch:  22\n","epoch:  23\n","epoch:  24\n","epoch:  25\n","epoch:  26\n","epoch:  27\n","epoch:  28\n","epoch:  29\n","epoch:  30\n","epoch:  31\n","epoch:  32\n","epoch:  33\n","epoch:  34\n","epoch:  35\n","epoch:  36\n","epoch:  37\n","epoch:  38\n","epoch:  39\n","epoch:  40\n","epoch:  41\n","epoch:  42\n","epoch:  43\n","epoch:  44\n","epoch:  45\n","epoch:  46\n","epoch:  47\n","epoch:  48\n","epoch:  49\n","epoch:  50\n","epoch:  51\n","epoch:  52\n","epoch:  53\n","epoch:  54\n","epoch:  55\n","epoch:  56\n","epoch:  57\n","epoch:  58\n","epoch:  59\n","epoch:  60\n","epoch:  61\n","epoch:  62\n","epoch:  63\n","epoch:  64\n","epoch:  65\n","epoch:  66\n","epoch:  67\n","epoch:  68\n","epoch:  69\n","epoch:  70\n","epoch:  71\n","epoch:  72\n","epoch:  73\n","epoch:  74\n","epoch:  75\n","epoch:  76\n","epoch:  77\n","epoch:  78\n","epoch:  79\n","epoch:  80\n","epoch:  81\n","epoch:  82\n","epoch:  83\n","epoch:  84\n","epoch:  85\n","epoch:  86\n","epoch:  87\n","epoch:  88\n","epoch:  89\n","epoch:  90\n","epoch:  91\n","epoch:  92\n","epoch:  93\n","epoch:  94\n","epoch:  95\n","epoch:  96\n","epoch:  97\n","epoch:  98\n","epoch:  99\n","epoch:  100\n","epoch:  101\n","epoch:  102\n","epoch:  103\n","epoch:  104\n","epoch:  105\n","epoch:  106\n","epoch:  107\n","epoch:  108\n","epoch:  109\n","epoch:  110\n","epoch:  111\n","epoch:  112\n","epoch:  113\n","epoch:  114\n","epoch:  115\n","epoch:  116\n","epoch:  117\n","epoch:  118\n","epoch:  119\n","epoch:  120\n","epoch:  121\n","epoch:  122\n","epoch:  123\n","epoch:  124\n","epoch:  125\n","epoch:  126\n","epoch:  127\n","epoch:  128\n","epoch:  129\n","epoch:  130\n","epoch:  131\n","epoch:  132\n","epoch:  133\n","epoch:  134\n","epoch:  135\n","epoch:  136\n","epoch:  137\n","epoch:  138\n","epoch:  139\n","epoch:  140\n","epoch:  141\n","epoch:  142\n","epoch:  143\n","epoch:  144\n","epoch:  145\n","epoch:  146\n","epoch:  147\n","epoch:  148\n","epoch:  149\n","epoch:  150\n","epoch:  151\n","epoch:  152\n","epoch:  153\n","epoch:  154\n","epoch:  155\n","epoch:  156\n","epoch:  157\n","epoch:  158\n","epoch:  159\n","epoch:  160\n","epoch:  161\n","epoch:  162\n","epoch:  163\n","epoch:  164\n","epoch:  165\n","epoch:  166\n","epoch:  167\n","epoch:  168\n","epoch:  169\n","epoch:  170\n","epoch:  171\n","epoch:  172\n","epoch:  173\n","epoch:  174\n","epoch:  175\n","epoch:  176\n","epoch:  177\n","epoch:  178\n","epoch:  179\n","epoch:  180\n","epoch:  181\n","epoch:  182\n","epoch:  183\n","epoch:  184\n","epoch:  185\n","epoch:  186\n","epoch:  187\n","epoch:  188\n","epoch:  189\n","epoch:  190\n","epoch:  191\n","epoch:  192\n","epoch:  193\n","epoch:  194\n","epoch:  195\n","epoch:  196\n","epoch:  197\n","epoch:  198\n","epoch:  199\n","120753\n","2\n","epoch:  0\n","epoch:  1\n","epoch:  2\n","epoch:  3\n","epoch:  4\n","epoch:  5\n","epoch:  6\n","epoch:  7\n","epoch:  8\n","epoch:  9\n","epoch:  10\n","epoch:  11\n","epoch:  12\n","epoch:  13\n","epoch:  14\n","epoch:  15\n","epoch:  16\n","epoch:  17\n","epoch:  18\n","epoch:  19\n","epoch:  20\n","epoch:  21\n","epoch:  22\n","epoch:  23\n","epoch:  24\n","epoch:  25\n","epoch:  26\n","epoch:  27\n","epoch:  28\n","epoch:  29\n","epoch:  30\n","epoch:  31\n","epoch:  32\n","epoch:  33\n","epoch:  34\n","epoch:  35\n","epoch:  36\n","epoch:  37\n","epoch:  38\n","epoch:  39\n","epoch:  40\n","epoch:  41\n","epoch:  42\n","epoch:  43\n","epoch:  44\n","epoch:  45\n","epoch:  46\n","epoch:  47\n","epoch:  48\n","epoch:  49\n","epoch:  50\n","epoch:  51\n","epoch:  52\n","epoch:  53\n","epoch:  54\n","epoch:  55\n","epoch:  56\n","epoch:  57\n","epoch:  58\n","epoch:  59\n","epoch:  60\n","epoch:  61\n","epoch:  62\n","epoch:  63\n","epoch:  64\n","epoch:  65\n","epoch:  66\n","epoch:  67\n","epoch:  68\n","epoch:  69\n","epoch:  70\n","epoch:  71\n","epoch:  72\n","epoch:  73\n","epoch:  74\n","epoch:  75\n","epoch:  76\n","epoch:  77\n","epoch:  78\n","epoch:  79\n","epoch:  80\n","epoch:  81\n","epoch:  82\n","epoch:  83\n","epoch:  84\n","epoch:  85\n","epoch:  86\n","epoch:  87\n","epoch:  88\n","epoch:  89\n","epoch:  90\n","epoch:  91\n","epoch:  92\n","epoch:  93\n","epoch:  94\n","epoch:  95\n","epoch:  96\n","epoch:  97\n","epoch:  98\n","epoch:  99\n","epoch:  100\n","epoch:  101\n","epoch:  102\n","epoch:  103\n","epoch:  104\n","epoch:  105\n","epoch:  106\n","epoch:  107\n","epoch:  108\n","epoch:  109\n","epoch:  110\n","epoch:  111\n","epoch:  112\n","epoch:  113\n","epoch:  114\n","epoch:  115\n","epoch:  116\n","epoch:  117\n","epoch:  118\n","epoch:  119\n","epoch:  120\n","epoch:  121\n","epoch:  122\n","epoch:  123\n","epoch:  124\n","epoch:  125\n","epoch:  126\n","epoch:  127\n","epoch:  128\n","epoch:  129\n","epoch:  130\n","epoch:  131\n","epoch:  132\n","epoch:  133\n","epoch:  134\n","epoch:  135\n","epoch:  136\n","epoch:  137\n","epoch:  138\n","epoch:  139\n","epoch:  140\n","epoch:  141\n","epoch:  142\n","epoch:  143\n","epoch:  144\n","epoch:  145\n","epoch:  146\n","epoch:  147\n","epoch:  148\n","epoch:  149\n","epoch:  150\n","epoch:  151\n","epoch:  152\n","epoch:  153\n","epoch:  154\n","epoch:  155\n","epoch:  156\n","epoch:  157\n","epoch:  158\n","epoch:  159\n","epoch:  160\n","epoch:  161\n","epoch:  162\n","epoch:  163\n","epoch:  164\n","epoch:  165\n","epoch:  166\n","epoch:  167\n","epoch:  168\n","epoch:  169\n","epoch:  170\n","epoch:  171\n","epoch:  172\n","epoch:  173\n","epoch:  174\n","epoch:  175\n","epoch:  176\n","epoch:  177\n","epoch:  178\n","epoch:  179\n","epoch:  180\n","epoch:  181\n","epoch:  182\n","epoch:  183\n","epoch:  184\n","epoch:  185\n","epoch:  186\n","epoch:  187\n","epoch:  188\n","epoch:  189\n","epoch:  190\n","epoch:  191\n","epoch:  192\n","epoch:  193\n","epoch:  194\n","epoch:  195\n","epoch:  196\n","epoch:  197\n","epoch:  198\n","epoch:  199\n","Finished Training\n"]}],"source":["loss_list = []\n","acc_list = []\n","train_err_list = []\n","test_err_list = []\n","\n","for trial in range(3):\n","    active_model = Net3()\n","    free_params = sum(p.numel() for p in active_model.parameters() if p.requires_grad)\n","    print(free_params)\n","\n","    trainstep = 125\n","    # Loss and optimizer\n","\n","    criterion = nn.NLLLoss() #You can modify the loss function\n","    optimizer = optim.SGD(active_model.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n","\n","    # Train the model\n","\n","    total_step = len(trainloader)\n","    print(total_step)\n","    trial_loss_list = []\n","    trial_acc_list = []\n","\n","    trial_train_err_list = []\n","    trial_test_err_list = []\n","\n","    num_epochs = 200\n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch: \", epoch)\n","        total = 0\n","        correct = 0\n","        randomly_selected = uncertainty_metric_max_min_difference_selection(active_model, trainloader)\n","        for i, (images, labels) in enumerate(trainloader):\n","            if i not in randomly_selected:\n","                continue\n","\n","    #         images = images.cuda(async=True)\n","    #         labels = labels.cuda(async=True)\n","\n","            images = Variable(images)\n","            labels = Variable(labels)\n","            # Run the forward pass\n","    #         print(\"running forward pass....\")\n","            outputs = active_model(images)\n","\n","            loss = criterion(outputs, labels)\n","            trial_loss_list.append(loss.item())\n","\n","            # Backprop \n","    #         print(\"backpropagating......\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","             # Track the accuracy\n","            total = labels.size(0) + total\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == labels).sum().item() + correct\n","            trial_acc_list.append(correct / total)\n","\n","\n","            if (i + 1) % trainstep == 0:\n","                w = torch.nn.utils.parameters_to_vector(active_model.parameters())\n","                print(w)\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                              (correct / total) * 100))\n","        trial_train_err_list.append(compute_train_acc(active_model, trainloader))\n","        trial_test_err_list.append(compute_test_acc(active_model, testloader))\n","        if (total == correct):\n","            break \n","\n","#     loss_list.append(np.array(trial_loss_list))\n","#     acc_list.append(trial_acc_list)\n","    train_err_list.append(trial_train_err_list)\n","    test_err_list.append(trial_test_err_list)\n","            \n","print('Finished Training') \n","\n","# loss_list = np.array(loss_list)\n","# acc_list = np.array(acc_list)\n","train_err_list= np.array(train_err_list)\n","test_err_list=np.array(test_err_list)\n","\n","# loss_list = np.mean(loss_list, axis=0)\n","# acc_list = np.mean(acc_list, axis=0)\n","train_err_list= np.mean(train_err_list, axis=0)\n","test_err_list=np.mean(test_err_list, axis=0)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qG0-lFRwzCWV"},"outputs":[],"source":["# testing = []\n","# testing.append([1,2,3])\n","# testing.append([4,2,3])\n","# testing = np.array(testing)\n","# print(np.mean(testing, axis=0))"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"2gX9aVpjzCWV","colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"status":"error","timestamp":1667402451956,"user_tz":-60,"elapsed":234,"user":{"displayName":"Koen Desplenter","userId":"12052997412530298636"}},"outputId":"4e6db10e-ba3d-44ad-8a8e-29de5b1d3baa"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-b924599d4297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ... after training, save your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'res_test2/active_model3_minimum.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# .. to load your previously training model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'res_test2/active_model3_minimum.pt'"]}],"source":["# ... after training, save your model \n","model_filename = 'res_test2/active_model3_minimum.pt'\n","torch.save(active_model.state_dict(), model_filename)\n","\n","# .. to load your previously training model:\n","# model2 = Net3()\n","# model2.load_state_dict(torch.load(model_filename))\n","# model2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_ZJAJDVzCWV"},"outputs":[],"source":["active_train_err_list = train_err_list\n","active_test_err_list = test_err_list\n","active_acc_list1 = acc_list\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3chLCxqJzCWW"},"outputs":[],"source":["pickle_out = open(\"res_test2/test1_largest_margin_minimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(active_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","\n","pickle_out = open(\"res_test2/test1_largest_margin_minimum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(active_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","# pickle_out = open(\"test1_random_testing_accuracy.pickle\",\"wb\")\n","# pickle.dump(new_rand_test_err_list, pickle_out)\n","# pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"MB8Gy6w4zCWW"},"source":["# Random Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0aBppYpzCWW"},"outputs":[],"source":["# random_model2 = Net3()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmSDuTb3zCWW"},"outputs":[],"source":["rand_loss_list = []\n","rand_acc_list = []\n","rand_train_err_list = []\n","rand_test_err_list = []\n","\n","for trial in range(3):\n","    random_model2 = Net3()\n","    free_params = sum(p.numel() for p in random_model2.parameters() if p.requires_grad)\n","\n","    trainstep = 125\n","    # Loss and optimizer\n","\n","    criterion = nn.NLLLoss() #You can modify the loss function\n","    optimizer = optim.SGD(random_model2.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n","\n","    # Train the model\n","\n","    total_step = len(trainloader)\n","    print(total_step)\n","    trial_rand_loss_list = []\n","    trial_rand_acc_list = []\n","\n","    trial_rand_train_err_list = []\n","    trial_rand_test_err_list = []\n","\n","    num_epochs = 200\n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch: \", epoch)\n","        total = 0\n","        correct = 0\n","        randomly_selected = np.random.choice(range(len(list(trainloader))), size=20)\n","        for i, (images, labels) in enumerate(trainloader):\n","            if i not in randomly_selected:\n","                continue\n","\n","    #         images = images.cuda(async=True)\n","    #         labels = labels.cuda(async=True)\n","\n","            images = Variable(images)\n","            labels = Variable(labels)\n","            # Run the forward pass\n","    #         print(\"running forward pass....\")\n","            outputs = random_model2(images)\n","\n","            loss = criterion(outputs, labels)\n","            rand_loss_list.append(loss.item())\n","\n","            # Backprop \n","    #         print(\"backpropagating......\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","             # Track the accuracy\n","            total = labels.size(0) + total\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == labels).sum().item() + correct\n","            rand_acc_list.append(correct / total)\n","\n","\n","            if (i + 1) % trainstep == 0:\n","                w = torch.nn.utils.parameters_to_vector(random_model2.parameters())\n","                print(w)\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                              (correct / total) * 100))\n","        trial_rand_train_err_list.append(compute_train_acc(random_model2, trainloader))\n","        trial_rand_test_err_list.append(compute_test_acc(random_model2, testloader))\n","        if (total == correct):\n","            break \n","\n","    rand_loss_list.append(trial_rand_loss_list)\n","    rand_acc_list.append(trial_rand_acc_list)\n","    rand_train_err_list.append(trial_rand_train_err_list)\n","    rand_test_err_list.append(trial_rand_test_err_list)\n","            \n","print('Finished Training') \n","\n","# rand_loss_list = np.array(rand_loss_list)\n","# rand_acc_list = np.array(rand_acc_list)\n","rand_train_err_list= np.array(rand_train_err_list)\n","rand_test_err_list=np.array(rand_test_err_list)\n","\n","# rand_loss_list = np.mean(rand_loss_list, axis=0)\n","# rand_acc_list = np.mean(rand_acc_list, axis=0)\n","rand_train_err_list= np.mean(rand_train_err_list, axis=0)\n","rand_test_err_list=np.mean(rand_test_err_list, axis=0)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JuEjBc91zCWX"},"source":["## Compare Largest Margin to Random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FA-z0iZUzCWX"},"outputs":[],"source":["new_active_train_err_list = [0]\n","new_active_train_err_list.extend(active_train_err_list)\n","plt.plot(new_active_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test3_4_acc1.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_largest_margin_minimum2_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_active_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test1_random2_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_train_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qn01yhqtzCWX"},"outputs":[],"source":["new_active_test_err_list = [0]\n","new_active_test_err_list.extend(active_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_active_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test3_4_acc2.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test2_largest_margin_minimum2_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_active_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test2_random2_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_test_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pk1WpgIgzCWY"},"outputs":[],"source":["# ... after training, save your model \n","model_filename = 'res_test2/random_model2.pt'\n","torch.save(random_model2.state_dict(), model_filename)\n","\n","# .. to load your previously training model:\n","# model2 = Net3()\n","# model2.load_state_dict(torch.load(model_filename))\n","# model2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIA1wS2OzCWY"},"outputs":[],"source":["plt.plot(active_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test2_acc1_min.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmi9tF4_zCWY"},"outputs":[],"source":["plt.plot(active_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test2_acc2_min.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr9fglzpzCWY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"spIHJgOczCWY"},"source":["# Second Diff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRq0p2tazCWY"},"outputs":[],"source":["# second_model1 = Net3()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta8S3EpozCWZ"},"outputs":[],"source":["# Find optimal_batch\n","def uncertainty_metric_max_second_difference_selection(model, trainloader):\n","    batch_evaluation = []\n","    for i, (images, labels) in enumerate(trainloader):\n","        outputs = model(images)\n","        batch_differences = []\n","        for j in range(len(outputs)):\n","            \n","#             min_prob = min(outputs[j])\n","            vals = outputs[j].data.numpy()\n","            top_two = vals[np.argsort(vals)[-2:]]\n","            \n","            diff = abs(float(top_two[0] - top_two[1]))\n","            batch_differences.append(diff)\n","        batch_mean = np.mean(batch_differences)\n","        batch_evaluation.append(batch_mean)\n","    batch_evaluation = np.array(batch_evaluation)\n","    k = 20\n","#     largest = list((-batch_evaluation).argsort()[:k])\n","    smallest = list(np.argsort(batch_evaluation)[:k])\n","    return smallest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PO-kC85wzCWZ","outputId":"ccd74a7c-2b58-4cc4-9ed9-5b6dc203c8ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["372\n","epoch:  0\n","epoch:  1\n","epoch:  2\n","epoch:  3\n","epoch:  4\n","epoch:  5\n","epoch:  6\n","epoch:  7\n","epoch:  8\n","epoch:  9\n","epoch:  10\n","epoch:  11\n","epoch:  12\n","epoch:  13\n","epoch:  14\n","epoch:  15\n","epoch:  16\n","epoch:  17\n","epoch:  18\n","epoch:  19\n"]},{"name":"stderr","output_type":"stream","text":["Process Process-1671:\n","Process Process-1672:\n","Process Process-1670:\n","Process Process-1669:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n","    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n","    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n","    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n","    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n","    if not self._poll(timeout):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n","    if not self._poll(timeout):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n","    if not self._poll(timeout):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n","    if not self._poll(timeout):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n","    return self._poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n","    return self._poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n","    return self._poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n","    r = wait([self], timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n","    r = wait([self], timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n","    return self._poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n","    r = wait([self], timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n","    ready = selector.select(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n","    ready = selector.select(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n","    ready = selector.select(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n","    r = wait([self], timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n","    ready = selector.select(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n","    fd_event_list = self._poll.poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n","    fd_event_list = self._poll.poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n","    fd_event_list = self._poll.poll(timeout)\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n","    fd_event_list = self._poll.poll(timeout)\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x109c59a58>>\n","Traceback (most recent call last):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n","    def __del__(self):\n","  File \"/Users/michellezhao/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n","    _error_if_any_worker_fails()\n","RuntimeError: DataLoader worker (pid 25795) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-b79e10bb092e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                               (correct / total) * 100))\n\u001b[1;32m     69\u001b[0m         \u001b[0mtrial_second_train_err_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_train_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtrial_second_test_err_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-5d4d2706fc7f>\u001b[0m in \u001b[0;36mcompute_test_acc\u001b[0;34m(model, testloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-41479c7eaa6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["second_loss_list = []\n","second_acc_list = []\n","second_train_err_list = []\n","second_test_err_list = []\n","\n","for trial in range(3):\n","    second_model1 = Net3()\n","    free_params = sum(p.numel() for p in second_model1.parameters() if p.requires_grad)\n","# print(free_params)\n","\n","    trainstep = 125\n","    # Loss and optimizer\n","\n","    criterion = nn.NLLLoss() #You can modify the loss function\n","    optimizer = optim.SGD(second_model1.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n","\n","    # Train the model\n","\n","    total_step = len(trainloader)\n","    print(total_step)\n","    trial_second_loss_list = []\n","    trial_second_acc_list = []\n","\n","    trial_second_train_err_list = []\n","    trial_second_test_err_list = []\n","\n","    num_epochs = 200\n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch: \", epoch)\n","        total = 0\n","        correct = 0\n","        randomly_selected = uncertainty_metric_max_second_difference_selection(second_model1, trainloader)\n","        for i, (images, labels) in enumerate(trainloader):\n","            if i not in randomly_selected:\n","                continue\n","\n","    #         images = images.cuda(async=True)\n","    #         labels = labels.cuda(async=True)\n","\n","            images = Variable(images)\n","            labels = Variable(labels)\n","            # Run the forward pass\n","    #         print(\"running forward pass....\")\n","            outputs = second_model1(images)\n","\n","            loss = criterion(outputs, labels)\n","            trial_second_loss_list.append(loss.item())\n","\n","            # Backprop \n","    #         print(\"backpropagating......\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","             # Track the accuracy\n","            total = labels.size(0) + total\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == labels).sum().item() + correct\n","            trial_second_acc_list.append(correct / total)\n","\n","\n","            if (i + 1) % trainstep == 0:\n","                w = torch.nn.utils.parameters_to_vector(second_model1.parameters())\n","                print(w)\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                              (correct / total) * 100))\n","        trial_second_train_err_list.append(compute_train_acc(second_model1, trainloader))\n","        trial_second_test_err_list.append(compute_test_acc(second_model1, testloader))\n","        if (total == correct):\n","            break \n","\n","    second_loss_list.append(trial_second_loss_list)\n","    second_acc_list.append(trial_second_acc_list)\n","    second_train_err_list.append(trial_second_train_err_list)\n","    second_test_err_list.append(trial_second_test_err_list)\n","            \n","print('Finished Training') \n","\n","second_loss_list = np.array(second_loss_list)\n","second_acc_list = np.array(second_acc_list)\n","second_train_err_list= np.array(second_train_err_list)\n","second_test_err_list=np.array(second_test_err_list)\n","\n","second_loss_list = np.mean(second_loss_list, axis=0)\n","second_acc_list = np.mean(second_acc_list, axis=0)\n","second_train_err_list= np.mean(second_train_err_list, axis=0)\n","second_test_err_list=np.mean(second_test_err_list, axis=0)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAdLNDEBzCWZ"},"outputs":[],"source":["# ... after training, save your model \n","model_filename = 'res_test2/second_model3_small.pt'\n","torch.save(second_model1.state_dict(), model_filename)\n","\n","# .. to load your previously training model:\n","# model2 = Net3()\n","# model2.load_state_dict(torch.load(model_filename))\n","# model2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkZ8ih_KzCWZ"},"outputs":[],"source":["new_second_train_err_list = [0]\n","new_second_train_err_list.extend(second_train_err_list)\n","plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test2_4_acc1.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_smallest_margin_minimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_second_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test1_random_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_train_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y9mtI6NzCWa"},"outputs":[],"source":["new_second_test_err_list = [0]\n","new_second_test_err_list.extend(second_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_second_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test2_4_acc2.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_smallest_margin_minimum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_second_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test1_random_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_test_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMUJ4zOLzCWa"},"outputs":[],"source":["plt.plot(second_test_err_list)\n","plt.plot(active_test_err_list)\n","# plt.plot(rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['smallest margin', 'largest margin', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test2_acc2.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI0QIDwJzCWa"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"V8rjgNcOzCWa"},"source":["# Compare Active, Second, Random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2cb1vbVzCWa"},"outputs":[],"source":["new_active_train_err_list = [0]\n","new_active_train_err_list.extend(active_train_err_list)\n","plt.plot(new_active_train_err_list)\n","\n","new_second_train_err_list = [0]\n","new_second_train_err_list.extend(second_train_err_list)\n","plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_4_acc1.png\")\n","plt.show()\n","\n","\n","pickle_out = open(\"res_test2/test2_largest_margin_minimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_active_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","\n","pickle_out = open(\"res_test2/test2_smallest_margin_minimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_second_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test2_random_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_train_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzQXU7RzzCWb"},"outputs":[],"source":["new_active_test_err_list = [0]\n","new_active_test_err_list.extend(active_test_err_list)\n","\n","new_second_test_err_list = [0]\n","new_second_test_err_list.extend(second_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_active_test_err_list)\n","\n","plt.plot(new_second_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_4_acc2.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test2_largest_margin_minimum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_active_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test2_smallest_margin_minimum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_second_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"res_test2/test2_random_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_rand_test_err_list, pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gnBmHfczCWb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-21X1ikYzCWb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GYZOINC0zCWb"},"source":["# Entropy Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyaE3BrVzCWb"},"outputs":[],"source":["# entropy_model1 = Net3()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mimUkY01zCWb"},"outputs":[],"source":["# Find optimal_batch\n","def uncertainty_metric_entropy_selection(model, trainloader):\n","    batch_evaluation = []\n","    for i, (images, labels) in enumerate(trainloader):\n","        outputs = model(images)\n","        batch_differences = []\n","        for j in range(len(outputs)):\n","            total = 0\n","            output_probs = outputs[j].data.numpy()\n","            for item in output_probs:\n","                entropy = -1* item * np.log(item)\n","                total += entropy\n","            batch_differences.append(total)\n","            \n","        batch_mean = np.mean(batch_differences)\n","        batch_evaluation.append(batch_mean)\n","    batch_evaluation = np.array(batch_evaluation)\n","    k = 20\n","    largest = list((-batch_evaluation).argsort()[:k])\n","#     smallest = list(np.argsort(batch_evaluation)[:k])\n","    return largest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZE5Lym1OzCWb"},"outputs":[],"source":["entropy_loss_list = []\n","entropy_acc_list = []\n","entropy_train_err_list = []\n","entropy_test_err_list = []\n","\n","for trial in range(3):\n","    entropy_model1 = Net3()\n","    free_params = sum(p.numel() for p in entropy_model1.parameters() if p.requires_grad)\n","    print(free_params)\n","\n","    trainstep = 125\n","    # Loss and optimizer\n","\n","    criterion = nn.NLLLoss() #You can modify the loss function\n","    optimizer = optim.SGD(entropy_model1.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n","\n","    # Train the model\n","\n","    total_step = len(trainloader)\n","    print(total_step)\n","    trial_entropy_loss_list = []\n","    trial_entropy_acc_list = []\n","\n","    trial_entropy_train_err_list = []\n","    trial_entropy_test_err_list = []\n","\n","    num_epochs = 200\n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch: \", epoch)\n","        total = 0\n","        correct = 0\n","        randomly_selected = uncertainty_metric_entropy_selection(entropy_model1, trainloader)\n","        for i, (images, labels) in enumerate(trainloader):\n","            if i not in randomly_selected:\n","                continue\n","\n","    #         images = images.cuda(async=True)\n","    #         labels = labels.cuda(async=True)\n","\n","            images = Variable(images)\n","            labels = Variable(labels)\n","            # Run the forward pass\n","    #         print(\"running forward pass....\")\n","            outputs = entropy_model1(images)\n","\n","            loss = criterion(outputs, labels)\n","            trial_entropy_loss_list.append(loss.item())\n","\n","            # Backprop \n","    #         print(\"backpropagating......\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","             # Track the accuracy\n","            total = labels.size(0) + total\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == labels).sum().item() + correct\n","            trial_entropy_acc_list.append(correct / total)\n","\n","\n","            if (i + 1) % trainstep == 0:\n","                w = torch.nn.utils.parameters_to_vector(entropy_model1.parameters())\n","                print(w)\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                              (correct / total) * 100))\n","        trial_entropy_train_err_list.append(compute_train_acc(entropy_model1, trainloader))\n","        trial_entropy_test_err_list.append(compute_test_acc(entropy_model1, testloader))\n","        if (total == correct):\n","            break \n","\n","    entropy_loss_list.append(trial_entropy_loss_list)\n","    entropy_acc_list.append(trial_entropy_acc_list)\n","    entropy_train_err_list.append(trial_entropy_train_err_list)\n","    entropy_test_err_list.append(trial_entropy_test_err_list)\n","            \n","print('Finished Training') \n","\n","entropy_loss_list = np.array(entropy_loss_list)\n","entropy_acc_list = np.array(entropy_acc_list)\n","entropy_train_err_list= np.array(entropy_train_err_list)\n","entropy_test_err_list=np.array(entropy_test_err_list)\n","\n","entropy_loss_list = np.mean(entropy_loss_list, axis=0)\n","entropy_acc_list = np.mean(entropy_acc_list, axis=0)\n","entropy_train_err_list= np.mean(entropy_train_err_list, axis=0)\n","entropy_test_err_list=np.mean(entropy_test_err_list, axis=0)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvR1ubB_zCWc"},"outputs":[],"source":["# ... after training, save your model \n","model_filename = 'res_test2/entropy_model1.pt'\n","torch.save(entropy_model1.state_dict(), model_filename)\n","\n","# .. to load your previously training model:\n","# model2 = Net3()\n","# model2.load_state_dict(torch.load(model_filename))\n","# model2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bpIbrW5zCWc"},"outputs":[],"source":["plt.plot(entropy_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test3_acc1.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7MqQf0NzCWc"},"outputs":[],"source":["plt.plot(entropy_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test3_acc2.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIMqHhZPzCWc"},"outputs":[],"source":["new_entropy_train_err_list = [0]\n","new_entropy_train_err_list.extend(entropy_train_err_list)\n","plt.plot(new_entropy_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test3_4_acc1.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_entropy_maxmimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_entropy_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","# pickle_out = open(\"test1_random_training_accuracy.pickle\",\"wb\")\n","# pickle.dump(new_rand_train_err_list, pickle_out)\n","# pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jfm9AJsfzCWd"},"outputs":[],"source":["new_entropy_test_err_list = [0]\n","new_entropy_test_err_list.extend(entropy_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_entropy_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test3_4_acc2.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_entropy_maximum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_entropy_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","# pickle_out = open(\"test1_random_testing_accuracy.pickle\",\"wb\")\n","# pickle.dump(new_rand_test_err_list, pickle_out)\n","# pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vccmsRPszCWd"},"outputs":[],"source":["new_active_train_err_list = [0]\n","new_active_train_err_list.extend(active_train_err_list)\n","plt.plot(new_active_train_err_list)\n","\n","new_second_train_err_list = [0]\n","new_second_train_err_list.extend(second_train_err_list)\n","plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","\n","new_entropy_train_err_list = [0]\n","new_entropy_train_err_list.extend(entropy_train_err_list)\n","plt.plot(new_entropy_train_err_list)\n","\n","\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin','entropy', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_6_acc1.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hChbbID5zCWd"},"outputs":[],"source":["new_active_test_err_list = [0]\n","new_active_test_err_list.extend(active_test_err_list)\n","\n","new_second_test_err_list = [0]\n","new_second_test_err_list.extend(second_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_active_test_err_list)\n","\n","plt.plot(new_second_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","\n","new_entropy_test_err_list = [0]\n","new_entropy_test_err_list.extend(entropy_test_err_list)\n","plt.plot(new_entropy_test_err_list)\n","\n","\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin','entropy',  'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_6_acc2.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Io88C-P2zCWd"},"source":["# Least Confident"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2H9xd9ZzCWd"},"outputs":[],"source":["# lc_model1 = Net3()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59jZXODZzCWd"},"outputs":[],"source":["# Find optimal_batch\n","def uncertainty_metric_lc_selection(model, trainloader):\n","    batch_evaluation = []\n","    for i, (images, labels) in enumerate(trainloader):\n","        outputs = model(images)\n","        batch_differences = []\n","        for j in range(len(outputs)):\n","            max_prob = max(outputs[j])\n","            diff = float(1 - max_prob)\n","            batch_differences.append(diff)\n","        batch_mean = np.sum(batch_differences)\n","        batch_evaluation.append(batch_mean)\n","    batch_evaluation = np.array(batch_evaluation)\n","    k = 20\n","    return list((-batch_evaluation).argsort()[:k])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"547tDKfgzCWe"},"outputs":[],"source":["lc_loss_list = []\n","lc_acc_list = []\n","lc_train_err_list = []\n","lc_test_err_list = []\n","\n","for trial in range(3):\n","    lc_model1 = Net3()\n","    free_params = sum(p.numel() for p in lc_model1.parameters() if p.requires_grad)\n","    print(free_params)\n","\n","    trainstep = 125\n","    # Loss and optimizer\n","\n","    criterion = nn.NLLLoss() #You can modify the loss function\n","    optimizer = optim.SGD(lc_model1.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n","\n","    # Train the model\n","\n","    total_step = len(trainloader)\n","    print(total_step)\n","    trial_lc_loss_list = []\n","    trial_lc_acc_list = []\n","\n","    trial_lc_train_err_list = []\n","    trial_lc_test_err_list = []\n","\n","    num_epochs = 200\n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch: \", epoch)\n","        total = 0\n","        correct = 0\n","        randomly_selected = uncertainty_metric_lc_selection(lc_model1, trainloader)\n","        for i, (images, labels) in enumerate(trainloader):\n","            if i not in randomly_selected:\n","                continue\n","\n","    #         images = images.cuda(async=True)\n","    #         labels = labels.cuda(async=True)\n","\n","            images = Variable(images)\n","            labels = Variable(labels)\n","            # Run the forward pass\n","    #         print(\"running forward pass....\")\n","            outputs = lc_model1(images)\n","\n","            loss = criterion(outputs, labels)\n","            trial_lc_loss_list.append(loss.item())\n","\n","            # Backprop \n","    #         print(\"backpropagating......\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","             # Track the accuracy\n","            total = labels.size(0) + total\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == labels).sum().item() + correct\n","            trial_lc_acc_list.append(correct / total)\n","\n","\n","            if (i + 1) % trainstep == 0:\n","                w = torch.nn.utils.parameters_to_vector(lc_model1.parameters())\n","                print(w)\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                              (correct / total) * 100))\n","        trial_lc_train_err_list.append(compute_train_acc(lc_model1, trainloader))\n","        trial_lc_test_err_list.append(compute_test_acc(lc_model1, testloader))\n","        if (total == correct):\n","            break \n","\n","    lc_loss_list.append(trial_lc_loss_list)\n","    lc_acc_list.append(trial_lc_acc_list)\n","    lc_train_err_list.append(trial_lc_train_err_list)\n","    lc_test_err_list.append(trial_lc_test_err_list)\n","            \n","print('Finished Training') \n","\n","lc_loss_list = np.array(lc_loss_list)\n","lc_acc_list = np.array(lc_acc_list)\n","lc_train_err_list= np.array(lc_train_err_list)\n","lc_test_err_list=np.array(lc_test_err_list)\n","\n","lc_loss_list = np.mean(lc_loss_list, axis=0)\n","lc_acc_list = np.mean(lc_acc_list, axis=0)\n","lc_train_err_list= np.mean(lc_train_err_list, axis=0)\n","lc_test_err_list=np.mean(lc_test_err_list, axis=0)\n"," \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lh1h7T5ZzCWe"},"outputs":[],"source":["# ... after training, save your model \n","model_filename = 'res_test2/lc_model1.pt'\n","torch.save(lc_model1.state_dict(), model_filename)\n","\n","# .. to load your previously training model:\n","# model2 = Net3()\n","# model2.load_state_dict(torch.load(model_filename))\n","# model2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlQryT_6zCWe"},"outputs":[],"source":["plt.plot(lc_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test4_acc1.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIdRdeovzCWe"},"outputs":[],"source":["plt.plot(lc_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test4_acc2.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FVx4FhgzCWe"},"outputs":[],"source":["new_lc_train_err_list = [0]\n","new_lc_train_err_list.extend(lc_train_err_list)\n","plt.plot(new_lc_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/test4_4_acc1.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_lc_maxmimum_training_accuracy.pickle\",\"wb\")\n","pickle.dump(new_entropy_train_err_list, pickle_out)\n","pickle_out.close()\n","\n","# pickle_out = open(\"test1_random_training_accuracy.pickle\",\"wb\")\n","# pickle.dump(new_rand_train_err_list, pickle_out)\n","# pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6SjC2GFzCWf"},"outputs":[],"source":["new_lc_test_err_list = [0]\n","new_lc_test_err_list.extend(lc_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_lc_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.legend(['active', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/test4_4_acc2.png\")\n","plt.show()\n","\n","pickle_out = open(\"res_test2/test1_lc_maximum_testing_accuracy.pickle\",\"wb\")\n","pickle.dump(new_entropy_test_err_list, pickle_out)\n","pickle_out.close()\n","\n","# pickle_out = open(\"test1_random_testing_accuracy.pickle\",\"wb\")\n","# pickle.dump(new_rand_test_err_list, pickle_out)\n","# pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JB6LbDbzCWf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"COrtKXzgzCWf"},"source":["# Compare All"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OO3wDV1zCWf"},"outputs":[],"source":["new_active_train_err_list = [0]\n","new_active_train_err_list.extend(active_train_err_list)\n","plt.plot(new_active_train_err_list)\n","\n","new_second_train_err_list = [0]\n","new_second_train_err_list.extend(second_train_err_list)\n","plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_train_err_list = [0]\n","new_rand_train_err_list.extend(rand_train_err_list)\n","plt.plot(new_rand_train_err_list)\n","\n","new_entropy_train_err_list = [0]\n","new_entropy_train_err_list.extend(entropy_train_err_list)\n","plt.plot(new_entropy_train_err_list)\n","\n","new_lc_train_err_list = [0]\n","new_lc_train_err_list.extend(lc_train_err_list)\n","plt.plot(new_lc_train_err_list)\n","\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin','entropy', 'least-confidence', 'random'])\n","plt.title(\"Active Training Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_5_acc1.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jszGgEnqzCWf"},"outputs":[],"source":["new_active_test_err_list = [0]\n","new_active_test_err_list.extend(active_test_err_list)\n","\n","new_second_test_err_list = [0]\n","new_second_test_err_list.extend(second_test_err_list)\n","\n","# plt.plot(new_second_train_err_list)\n","# plt.plot(active_test_err_list)\n","new_rand_test_err_list = [0]\n","new_rand_test_err_list.extend(rand_test_err_list)\n","\n","plt.plot(new_active_test_err_list)\n","\n","plt.plot(new_second_test_err_list)\n","# plt.plot(active_test_err_list)\n","plt.plot(new_rand_test_err_list)\n","\n","new_entropy_test_err_list = [0]\n","new_entropy_test_err_list.extend(entropy_test_err_list)\n","plt.plot(new_entropy_test_err_list)\n","\n","new_lc_test_err_list = [0]\n","new_lc_test_err_list.extend(lc_test_err_list)\n","plt.plot(new_lc_test_err_list)\n","\n","# plt.plot(active_test_err_list)\n","plt.legend(['largest margin', 'smallest margin','entropy', 'least-confidence', 'random'])\n","plt.title(\"Active Test Set Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Test Set Accuracy\")\n","plt.savefig(\"res_test2/comparisontest2_5_acc2.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJWeQCR8zCWg"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}